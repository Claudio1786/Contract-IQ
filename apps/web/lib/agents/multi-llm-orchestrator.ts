// Contract IQ Multi-LLM Orchestration System\n// Advanced agent architecture supporting Google Gemini + OpenAI models\n\nimport { GoogleGenerativeAI } from '@google/generative-ai';\nimport OpenAI from 'openai';\n\n// Multi-LLM Provider Types\nexport type LLMProvider = 'gemini' | 'openai';\nexport type GeminiModel = 'gemini-1.5-pro' | 'gemini-1.5-flash' | 'text-embedding-004';\nexport type OpenAIModel = 'gpt-4o' | 'gpt-4o-mini' | 'gpt-3.5-turbo' | 'text-embedding-3-large' | 'text-embedding-3-small';\n\nexport type ModelType = GeminiModel | OpenAIModel;\n\n// Enhanced Agent Types with LLM Strategy\nexport type AgentType = \n  | 'clause_extraction'\n  | 'risk_scoring'\n  | 'benchmarking'\n  | 'negotiation_strategy'\n  | 'simulation'\n  | 'reporting'\n  | 'cross_validation'  // New: Multi-model validation\n  | 'cost_optimization'; // New: OpenAI for financial analysis\n\nexport type AgentPriority = 'critical' | 'high' | 'medium' | 'low';\nexport type AgentStatus = 'idle' | 'processing' | 'completed' | 'failed';\n\n// LLM Routing Strategy - Which model is best for each task\nexport interface LLMStrategy {\n  primary: {\n    provider: LLMProvider;\n    model: ModelType;\n    reason: string;\n  };\n  fallback: {\n    provider: LLMProvider;\n    model: ModelType;\n    reason: string;\n  };\n  crossValidation?: {\n    provider: LLMProvider;\n    model: ModelType;\n    threshold: number; // Confidence threshold to trigger cross-validation\n  };\n}\n\n// Optimized model routing based on our competitive analysis\nexport const MULTI_LLM_STRATEGIES: Record<AgentType, LLMStrategy> = {\n  // Clause Extraction: Gemini Flash for speed, OpenAI GPT-4o for accuracy\n  clause_extraction: {\n    primary: {\n      provider: 'gemini',\n      model: 'gemini-1.5-flash',\n      reason: 'Superior speed for structured extraction, optimized for JSON output'\n    },\n    fallback: {\n      provider: 'openai',\n      model: 'gpt-4o-mini',\n      reason: 'Reliable structured output when Gemini fails'\n    },\n    crossValidation: {\n      provider: 'openai',\n      model: 'gpt-4o',\n      threshold: 0.8 // If Gemini confidence < 80%, validate with OpenAI\n    }\n  },\n  \n  // Risk Scoring: OpenAI GPT-4o for complex reasoning, Gemini Pro for legal context\n  risk_scoring: {\n    primary: {\n      provider: 'openai',\n      model: 'gpt-4o',\n      reason: 'Superior reasoning for multi-dimensional risk assessment'\n    },\n    fallback: {\n      provider: 'gemini',\n      model: 'gemini-1.5-pro',\n      reason: 'Strong legal document understanding as backup'\n    },\n    crossValidation: {\n      provider: 'gemini',\n      model: 'gemini-1.5-pro',\n      threshold: 0.75 // Critical decisions require dual validation\n    }\n  },\n  \n  // Benchmarking: Gemini Pro for data analysis, OpenAI for market insights\n  benchmarking: {\n    primary: {\n      provider: 'gemini',\n      model: 'gemini-1.5-pro',\n      reason: 'Excellent at processing large datasets and patterns'\n    },\n    fallback: {\n      provider: 'openai',\n      model: 'gpt-4o',\n      reason: 'Strong analytical reasoning for market comparisons'\n    }\n  },\n  \n  // Negotiation Strategy: OpenAI GPT-4o for creative strategy, Gemini for tactical analysis\n  negotiation_strategy: {\n    primary: {\n      provider: 'openai',\n      model: 'gpt-4o',\n      reason: 'Superior creative reasoning and strategic thinking'\n    },\n    fallback: {\n      provider: 'gemini',\n      model: 'gemini-1.5-pro',\n      reason: 'Strong analytical backup for strategy validation'\n    }\n  },\n  \n  // Simulation: OpenAI for scenario modeling, Gemini for outcome prediction\n  simulation: {\n    primary: {\n      provider: 'openai',\n      model: 'gpt-4o',\n      reason: 'Advanced scenario modeling and probabilistic reasoning'\n    },\n    fallback: {\n      provider: 'gemini',\n      model: 'gemini-1.5-pro',\n      reason: 'Reliable outcome prediction based on historical patterns'\n    }\n  },\n  \n  // Reporting: Gemini Pro for clear writing, OpenAI for executive communication\n  reporting: {\n    primary: {\n      provider: 'gemini',\n      model: 'gemini-1.5-pro',\n      reason: 'Excellent at clear, structured business writing'\n    },\n    fallback: {\n      provider: 'openai',\n      model: 'gpt-4o',\n      reason: 'Strong executive-level communication when needed'\n    }\n  },\n  \n  // Cross Validation: Always use the alternative model for verification\n  cross_validation: {\n    primary: {\n      provider: 'openai',\n      model: 'gpt-4o',\n      reason: 'Independent validation of Gemini primary results'\n    },\n    fallback: {\n      provider: 'gemini',\n      model: 'gemini-1.5-pro',\n      reason: 'Secondary validation when OpenAI unavailable'\n    }\n  },\n  \n  // Cost Optimization: OpenAI excels at financial analysis and optimization\n  cost_optimization: {\n    primary: {\n      provider: 'openai',\n      model: 'gpt-4o',\n      reason: 'Superior financial reasoning and cost-benefit analysis'\n    },\n    fallback: {\n      provider: 'gemini',\n      model: 'gemini-1.5-pro',\n      reason: 'Analytical backup for cost calculations'\n    }\n  }\n};\n\n// Model Configuration for Different Providers\nexport interface ModelConfig {\n  temperature: number;\n  topP?: number;\n  maxTokens: number;\n  systemMessage: string;\n  responseFormat?: 'json' | 'text';\n}\n\n// Multi-LLM Client Manager\nexport class MultiLLMClient {\n  private gemini: GoogleGenerativeAI;\n  private openai: OpenAI;\n  private costTracker = new Map<string, number>(); // Track costs per contract\n  \n  constructor(geminiApiKey: string, openaiApiKey: string) {\n    this.gemini = new GoogleGenerativeAI(geminiApiKey);\n    this.openai = new OpenAI({ apiKey: openaiApiKey });\n  }\n  \n  async callModel(\n    provider: LLMProvider,\n    model: ModelType,\n    config: ModelConfig,\n    prompt: string,\n    contractId: string\n  ): Promise<{ content: string; cost: number }> {\n    const startTime = Date.now();\n    \n    try {\n      if (provider === 'gemini') {\n        return await this.callGemini(model as GeminiModel, config, prompt, contractId);\n      } else {\n        return await this.callOpenAI(model as OpenAIModel, config, prompt, contractId);\n      }\n    } catch (error) {\n      console.error(`Model call failed: ${provider}:${model}`, error);\n      throw error;\n    } finally {\n      const duration = Date.now() - startTime;\n      console.log(`${provider}:${model} call took ${duration}ms for contract ${contractId}`);\n    }\n  }\n  \n  private async callGemini(\n    model: GeminiModel,\n    config: ModelConfig,\n    prompt: string,\n    contractId: string\n  ): Promise<{ content: string; cost: number }> {\n    const genModel = this.gemini.getGenerativeModel({\n      model,\n      generationConfig: {\n        temperature: config.temperature,\n        topP: config.topP || 0.8,\n        maxOutputTokens: config.maxTokens,\n        responseMimeType: config.responseFormat === 'json' ? 'application/json' : 'text/plain'\n      },\n      systemInstruction: config.systemMessage\n    });\n    \n    const result = await genModel.generateContent(prompt);\n    const response = result.response;\n    const text = response.text();\n    \n    // Estimate cost (Google pricing: ~$0.35 per 1M tokens)\n    const estimatedTokens = text.length / 4; // Rough token estimate\n    const cost = (estimatedTokens / 1000000) * 0.35;\n    \n    this.updateCostTracking(contractId, cost);\n    \n    return { content: text, cost };\n  }\n  \n  private async callOpenAI(\n    model: OpenAIModel,\n    config: ModelConfig,\n    prompt: string,\n    contractId: string\n  ): Promise<{ content: string; cost: number }> {\n    const messages: OpenAI.Chat.Completions.ChatCompletionMessageParam[] = [\n      { role: 'system', content: config.systemMessage },\n      { role: 'user', content: prompt }\n    ];\n    \n    const completion = await this.openai.chat.completions.create({\n      model,\n      messages,\n      temperature: config.temperature,\n      max_tokens: config.maxTokens,\n      response_format: config.responseFormat === 'json' ? { type: 'json_object' } : undefined\n    });\n    \n    const content = completion.choices[0]?.message?.content || '';\n    \n    // Calculate actual cost based on OpenAI pricing\n    const inputTokens = completion.usage?.prompt_tokens || 0;\n    const outputTokens = completion.usage?.completion_tokens || 0;\n    \n    let cost = 0;\n    if (model === 'gpt-4o') {\n      cost = (inputTokens * 0.0025 + outputTokens * 0.01) / 1000; // GPT-4o pricing\n    } else if (model === 'gpt-4o-mini') {\n      cost = (inputTokens * 0.00015 + outputTokens * 0.0006) / 1000; // GPT-4o-mini pricing\n    }\n    \n    this.updateCostTracking(contractId, cost);\n    \n    return { content, cost };\n  }\n  \n  private updateCostTracking(contractId: string, cost: number): void {\n    const currentCost = this.costTracker.get(contractId) || 0;\n    this.costTracker.set(contractId, currentCost + cost);\n  }\n  \n  getTotalCost(contractId: string): number {\n    return this.costTracker.get(contractId) || 0;\n  }\n  \n  getAllCosts(): Map<string, number> {\n    return new Map(this.costTracker);\n  }\n}\n\n// Enhanced Input/Output Types\nexport interface AgentInput {\n  contractId: string;\n  contractText?: string;\n  extractedClauses?: any[];\n  riskAssessment?: any;\n  benchmarkData?: any;\n  context: ProcessingContext;\n  metadata: AgentMetadata;\n  preferences?: ModelPreferences;\n}\n\nexport interface ModelPreferences {\n  preferredProvider?: LLMProvider;\n  requireCrossValidation?: boolean;\n  maxCostPerRequest?: number;\n  prioritizeSpeed?: boolean;\n  prioritizeAccuracy?: boolean;\n}\n\nexport interface ProcessingContext {\n  companyProfile?: any;\n  contractType: string;\n  urgency: 'low' | 'medium' | 'high' | 'critical';\n  userId: string;\n  sessionId: string;\n  objectives: string[];\n}\n\nexport interface AgentMetadata {\n  version: string;\n  modelVersion: string;\n  processingNode: string;\n  retryCount: number;\n  dependencies: AgentType[];\n}\n\nexport interface AgentOutput {\n  agentId: string;\n  contractId: string;\n  result: any;\n  confidence: number;\n  sources: any[];\n  processingTime: number;\n  timestamp: Date;\n  modelUsed: {\n    provider: LLMProvider;\n    model: ModelType;\n    cost: number;\n  };\n  crossValidation?: ValidationResult;\n  errors?: string[];\n  warnings?: string[];\n}\n\nexport interface ValidationResult {\n  isValid: boolean;\n  confidence: number;\n  differences: string[];\n  recommendation: 'accept' | 'review' | 'reject';\n  validator: {\n    provider: LLMProvider;\n    model: ModelType;\n  };\n}\n\nexport interface ContractProcessingInput {\n  contractId: string;\n  contractText: string;\n  context: ProcessingContext;\n  requiredAgents?: AgentType[];\n  priority?: 'critical' | 'high' | 'medium' | 'low';\n  preferences?: ModelPreferences;\n}\n\nexport interface ProcessingJob {\n  id: string;\n  contractId: string;\n  requiredAgents: AgentType[];\n  status: 'queued' | 'processing' | 'completed' | 'failed';\n  priority: 'critical' | 'high' | 'medium' | 'low';\n  context: ProcessingContext;\n  createdAt: Date;\n  completedAt?: Date;\n  results: Map<AgentType, AgentOutput>;\n  error?: string;\n}\n\nexport interface ProcessingResult {\n  jobId: string;\n  contractId: string;\n  status: 'completed' | 'failed';\n  agentResults: Record<string, AgentOutput>;\n  processingTime: number;\n  confidence: number;\n  summary: ExecutiveSummary;\n  error?: string;\n}\n\nexport interface ExecutiveSummary {\n  overview: string;\n  keyRisks: string[];\n  opportunities: string[];\n  recommendations: string[];\n  nextSteps: string[];\n  confidence: number;\n}\n\n// Enhanced result interfaces\nexport interface EnhancedProcessingResult extends ProcessingResult {\n  modelMetrics: {\n    totalCost: number;\n    modelsUsed: Record<string, number>;\n    crossValidationResults: CrossValidationSummary;\n  };\n}\n\nexport interface CrossValidationSummary {\n  totalValidations: number;\n  successfulValidations: number;\n  validationRate: number;\n  flaggedDifferences: string[];\n}\n\nexport interface CostAnalysis {\n  totalCost: number;\n  breakdown: Record<string, number>;\n  recommendations: string[];\n}\n\nexport default MultiLLMClient;\n"